project.class.name: Users

connector {

    name: auth-connector

    database {
        hostname: mysql
        port: 3306
        user: root
        password: password
        server_id: 1
        db: auth
    }

    kafka {
        topic_prefix: dbserver1
        bootstrap_servers: "kafka:9092"
    }

}

hudi {

    app_name: auth_users_rti

    hive.metastore.uris: "thrift://localhost:9083"

    kafka {
        input {
            bootstrap_servers: "localhost:29092"
            topic: dbserver1.auth.users
            max_trigger_offsets: 1000
            acl: false
        }
        trigger_interval: "5 seconds"
    }

    config {
        hoodie.combine.before.upsert: true
        hoodie.insert.shuffle.parallelism: 100
        hoodie.upsert.shuffle.parallelism: 100
        hoodie.table.name: hudi_auth_users_cow # For using Spark directly, if using HoodieDeltaStreamer, hoodie.datasource.write.table.name is used
        hoodie.datasource.write.table.type: COPY_ON_WRITE
        precombine.field: ts # precombine record with the same record key before actual write
        hoodie.datasource.write.recordkey.field: username
        hoodie.datasource.write.partitionpath.field: country # partition
        path: "file:///tmp/hudi_auth_users_cow" # hdfs path "hdfs://localhost:8020/user/hudi/hudi_auth_users_cow"
        checkpointLocation: "file:///tmp/checkpoints/hudi_auth_users_cow" # hdfs path "hdfs://localhost:8020/user/checkpoint/hudi_auth_users_cow"
        hoodie.keep.max.commit: 3 # controls how many commits can exist in the table at the same time
        hoodie.keep.min.commit: 2
        hoodie.cleaner.policy: "KEEP_LATEST_COMMITS"
        hoodie.cleaner.commits.retained: 3 # a minimum number of commits are always retained, even if they are older than hoodie.keep.max.commits
    }

}