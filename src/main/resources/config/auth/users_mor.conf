project.class.name: Users

connector {

    name: auth-connector

    database {
        hostname: mysql
        port: 3306
        user: root
        password: password
        server_id: 1
        db: auth
    }

    kafka {
        topic_prefix: dbserver1
        bootstrap_servers: "kafka:9092"
    }

}

hudi {

    app_name: auth_users_rti

    hive.metastore.uris: "thrift://localhost:9083"

    kafka {
        input {
            bootstrap_servers: "localhost:29092"
            topic: dbserver1.auth.users
            max_trigger_offsets: 1000
            acl: false
        }
        trigger_interval: "5 seconds"
    }

    config {
        hoodie.combine.before.upsert: true
        hoodie.insert.shuffle.parallelism: 100
        hoodie.upsert.shuffle.parallelism: 100
        hoodie.table.name: hudi_auth_users_mor # For using Spark directly, if using HoodieDeltaStreamer, hoodie.datasource.write.table.name is used
        hoodie.datasource.write.table.type: MERGE_ON_READ
        precombine.field: ts # precombine record with the same record key before actual write
        hoodie.datasource.write.recordkey.field: username
        hoodie.datasource.write.partitionpath.field: country # partition
        hoodie.datasource.write.hive_style_partitioning: true # set hive partition style: hudi -> SG, hive -> grass_region=SG
        path: "file:///tmp/hudi_auth_users_mor" # hdfs path "hdfs://localhost:8020/user/hudi/hudi_auth_users_mor"
        checkpointLocation: "file:///tmp/checkpoints/hudi_auth_users_mor" # hdfs path "hdfs://localhost:8020/user/checkpoint/hudi_auth_users_mor"
		hoodie.keep.max.commits: 3 # controls how many commits can exist in the table at the same time and if # commit > this number, cleaning process triggered
		hoodie.keep.min.commits: 2
		hoodie.cleaner.policy: "KEEP_LATEST_COMMITS"
		hoodie.cleaner.commits.retained: 1 # a minimum number of commits are always retained after cleaning process
		hoodie.compact.inline.max.delta.commits: 2
		compaction.async.enable: true
	}

}